{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRadDani/ONNX_inference_for_multiple_targets/blob/main/ONNX_Inference_different_targets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install onnxruntime-gpu onnx onnxconverter_common==1.8.1 pillow --quiet"
      ],
      "metadata": {
        "id": "WAAvq-meJPHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, datasets, transforms as T\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "X1McNEFeJot8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Download ImageNet Labels\n",
        "!curl -o imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "\n",
        "# Read the categories\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "  categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Export the model to ONNX\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
        "torch_out = resnet50(x)\n",
        "torch.onnx.export(resnet50,                     # model being run\n",
        "                  x,                            # model input (or a tuple for multiple inputs)\n",
        "                  \"resnet50.onnx\",              # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,           # store the trained parameter weights inside the model file\n",
        "                  opset_version=12,             # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,     # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],      # the model's input names\n",
        "                  output_names = ['output'])    # the model's output names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgduXQ30Jtwt",
        "outputId": "b4ea8058-c4d9-46fc-b050-854fd867f8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 10472  100 10472    0     0   161k      0 --:--:-- --:--:-- --:--:--  162k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50.eval()\n",
        "filename = '/content/cat.jpg'\n",
        "\n",
        "input_image = Image.open(filename)\n",
        "preprocess = T.Compose([\n",
        "  T.Resize(256),\n",
        "  T.CenterCrop(224),\n",
        "  T.ToTensor(),\n",
        "  T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "print(\"GPU Availability: \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  input_batch = input_batch.to('cuda')\n",
        "  resnet50.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wycnS7EwK0PJ",
        "outputId": "2df3454f-9f81-4386-f4e3-acca5e7a33ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Availability:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference ResNet-50 ONNX Model with ONNX Runtime"
      ],
      "metadata": {
        "id": "Djih87tDL2fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "from onnx import numpy_helper\n",
        "import time\n",
        "\n",
        "session_fp32 = onnxruntime.InferenceSession(\"resnet50.onnx\", providers=[\"OpenVINOExecutionProvider\"])\n",
        "\n",
        "def softmax(x):\n",
        "  \"\"\"\n",
        "    Compute softmax values for each set of scores in x.\n",
        "  \"\"\"\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "latency = []\n",
        "def run_sample(session, image_file, categories, inputs):\n",
        "  start = time.time()\n",
        "  input_arr = inputs.cpu().detach().numpy()\n",
        "  ort_outputs = session.run(None, {'input': input_arr})[0]\n",
        "  latency.append(time.time() - start)\n",
        "  output = ort_outputs.flatten()\n",
        "  output = softmax(output)\n",
        "  top5_catid = np.argsort(-output)[:5]\n",
        "  for catid in top5_catid:\n",
        "        print(categories[catid], output[catid])\n",
        "  return ort_outputs\n",
        "\n",
        "\n",
        "ort_output = run_sample(session_fp32, filename, categories, input_batch)\n",
        "print(f\"ONNX Runtime CPU_GPU/OpenVINO Inference Time = \\\n",
        "{(sum(latency) * 1000)/ len(latency):2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phEVWeB_L0q1",
        "outputId": "ed8d3e9c-fc46-4f0a-dec8-bf670aa95f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.20865938\n",
            "Egyptian cat 0.2022892\n",
            "lynx 0.19073758\n",
            "tiger cat 0.15811343\n",
            "hamper 0.02069059\n",
            "ONNX Runtime CPU_GPU/OpenVINO Inference Time = 69.701195 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with OpenVINO\n"
      ],
      "metadata": {
        "id": "3p9d8LutWGJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openvino-dev --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g-CZoBjWa_c",
        "outputId": "423a4f00-ddf2-4180-9b75-d65cdd4edd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference with OpenVINO\n",
        "from openvino.runtime import Core\n",
        "\n",
        "ei = Core()\n",
        "onnx_model_path = \"./resnet50.onnx\"\n",
        "model_onnx = ei.read_model(onnx_model_path)\n",
        "compiled_model = ei.compile_model(model=model_onnx, device_name=\"CPU\")\n",
        "\n",
        "# Inference\n",
        "output_layer = next(iter(compiled_model.outputs))\n",
        "\n",
        "letency = []\n",
        "input_arr = input_batch.cpu().detach().numpy()\n",
        "inputs = {'input':input_arr}\n",
        "start = time.time()\n",
        "request = compiled_model.create_infer_request()\n",
        "output = request.infer(inputs=inputs)\n",
        "\n",
        "outputs = request.get_output_tensor(output_layer.index).data\n",
        "output = outputs.flatten()\n",
        "output = softmax(output)\n",
        "top5_catid = np.argsort(-output)[:5]\n",
        "for catid in top5_catid:\n",
        "      print(categories[catid], output[catid])\n",
        "\n",
        "latency.append(time.time() - start)\n",
        "\n",
        "print(\"OpenVINO CPU Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))\n",
        "\n",
        "print(\"***** Verifying correctness *****\")\n",
        "for i in range(2):\n",
        "    print('OpenVINO and ONNX Runtime output {} are close:'.format(i), np.allclose(ort_output, outputs, rtol=1e-05, atol=1e-04))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flbK1CjHUFem",
        "outputId": "fb0908fb-0ff1-4b25-b4db-5ac9ab440851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.20865975\n",
            "Egyptian cat 0.20229013\n",
            "lynx 0.19073662\n",
            "tiger cat 0.1581131\n",
            "hamper 0.020690616\n",
            "OpenVINO CPU Inference time = 74.33 ms\n",
            "***** Verifying correctness *****\n",
            "OpenVINO and ONNX Runtime output 0 are close: True\n",
            "OpenVINO and ONNX Runtime output 1 are close: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APKN4C6pZzM9",
        "outputId": "aa830136-3b85-4cbb-b578-0d5258566bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}